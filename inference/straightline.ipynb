{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Straight Line\n",
    "\n",
    "_Phil Marshall, Daniel Foreman-Mackey and Dustin Lang_\n",
    "\n",
    "_Astro Hack Week, New York, September 2015_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals:\n",
    "\n",
    "* Set up and carry out a simple Bayesian inference, characterizing a simple posterior PDF\n",
    "\n",
    "\n",
    "* Compare brute force, analytic and MCMC sampled results\n",
    "\n",
    "\n",
    "* Check models, with posterior predictive distributions of test statistics\n",
    "\n",
    "\n",
    "* Use the Bayesian Evidence to compare various models, and understand its properties\n",
    "\n",
    "\n",
    "* Suggest some simple hacks to get some practice with Bayesian inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "plt.rcParams['savefig.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from straightline_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "* Let's generate a simple dataset: observations of $y$ with reported uncertainties $\\sigma_y$, at given $x$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAK9CAYAAAAt0QTlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3X+UZWdd5/vPl19pYEwzInSoK50QIBEcvNh9GYwXgR5R\no0sHe1ScHjKA3juIAebSygiCmgw4upZGmjHAYAbG6AJKGKBUZiRBhkJgkAhp1CUmiORHIU13iGAn\nhnQC5rl/nFPt6Up1dXV3nTqn6nm91qrVXfvsOvWczUnzrl3Pfna11gIAAD24z6QHAAAA60X8AgDQ\nDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AkyJqnpeVd1TVdtP42t3jGFc\n91TVJct8r5MeJ8CkiV+AJFX1hKp6Z1XdVFV3VtXfVNX7qupFS/Z7RVU9c4xDGfs956vq4qp67kl+\n2dJxjX2cAOMgfoHuVdW3J/lEkickuSLJC5P81yT3JPn3S3Z/RZJxxe9vJ3lga21hTM+/6OIkzzuN\nr1+vcQKsuftNegAAU+CVSb6c5EmttdtGH6iqb1iyb0tSq3nSqnpwa+2O1Q6itXZPkrtXu/+kbJRx\nAizHmV+A5NFJPrU0fJOktXbr4t+r6p4kD07y3OGc13uq6r8NH7t0+PnjquptVfWlJB8ePvYtVXVl\nVd0wnFLxhap6c1V9/ej3Wm4ubVXdZ/jcB6rqjqr6QFU9fjg94zeXeS1bquo1VfXFqvr7qnr3aMBX\n1U1JHp/kaSOvYf5kDtZxxnlTVb2nqp5SVX8yfJ2frap/u8zXP6SqXltVn6uqI1X1mar6mapa1Q8V\nAKfDmV+A5KYkF1TVN7fWPrXCfv82yZuSXJPB9Igk+eySff57kr9K8rP5xzPEz0jyqCRvTnIwyT9L\n8vwk35zk204wtl9O8h+S/H6Sq5M8MclVSbZk+Xm3lyf5UpJLht/zJUlel+RfDx///4b73J7kPw23\nHTrBGFajJXlMBq//TUl+M8n/k+TKqrq2tfaXSVJVD0ryR0kekeQ3kiwk+b+Hr/MRSfauwVgAjkv8\nAiSXJXlvkj+tqo9ncMb2/UnmW2tfW9yptfbWqnpjkhtaa287znP9aWvtoiXb3tBae83ohqr6WJLZ\nqnpKa+0jyz1RVW1L8lNJ5lprPzSy/ReSXHqc739ra+17Rva9T5J/X1Vf11q7vbX2e1X1n5LcssJr\nOBWV5Pwk39Fa+9/D7/3fk3wuyY9lEPAZvp5zkzyxtbb4g8N/raoDSf5DVf1aa+1v1nBcAMcw7QHo\nXmvt/UkuyODs6rdkEGpXJ/l8Vf3AST7dG5d5/iOLf6+qLcNpCNcMN33rCs/1nUnum+QNS7ZfvsLX\nXLHk848Mn+PsFb5mrXxqMXyTo1NGPp3BGehFP5LkQ0n+rqq+YfEjyf8ajvOp6zBOoGPO/AIkaa19\nIskPVdX9MphasDuDX8G/s6qe2Fq7bpVPdePSDcO5vZdkMPXgYUse3rrCcy0G618vGeuXq+rLx/ma\npSswLO73T1f4PmtludUf/m7J935sBqtqfHGZfVvufXwA1pT4BRgxnObwiSSfqKq/ymDu6o8kedUq\nn+LOZba9I4Mzy7+S5E+T/H0GZzmvyqn/Bu54F4f9w0nuv5ZW870ryfsyOBbL+cyajghgCfELcHzX\nDv88a2TbSd3coar+aZJ/keQXWmu/OLL9sav48puHfz525O+pqocmecjJjGOJSd6g4rNJvq619oEJ\njgHomDm/QPeqatdxHvq+4Z+fHtl2R05uCsHi2dCl/96+ZBVf+/4kX0vyk0u2v2iZfU/Gyb6GtfSO\nDFbW+O6lDwyXQLvvBMYEdMSZX4Dk8qp6YJK5DEL3AUm+PcmzMpjDO7qe7rVJnlFVe5N8IYOVH/7k\neE/cWrutqj6U5Geq6v5JDiT57iTnnGhQrbVbquo/J/npqvq9DC7C+z+TfG+SW3PqZ3A/keQnq+qV\nGZyJPdRaO6m1fk/S6LSHX03yL5P8j6q6Msn+DNZOfkKSH8pgnvOXxjgWoHPiFyD56Qzm9X5fBuvv\nPiCDaQavT/KLS25+8VMZrKjwi0kemOTKJIvxe7wY/TcZrNDwwgxC8OoMAvbAMvsufY6XJflKkn+X\nwXrBH0vyPRmsmHBkyb7H+/5Lt78qg8j8mSRfl+SDSU42fpc+50rf++hjrbU7q+ppGdwm+keSPCfJ\nbRn80PELw78DjE21NsmpXwCcrKp6SAZnR1/ZWvvlSY8HYCOZ6JzfqvrZqvp4Vd1WVYeqaq6qzluy\nz5Ujt+Bc/PiDJftsqarXV9WtVXV7Vb2zqh6+vq8GYO1V1ZZlNi/OF/7gOg4FYFOY6Jnfqnpvktkk\nH09y/yS/lMFtPx/fWvvKcJ/fTPLwDO4QtOiu1trhkef5Lxn8uvK5GfzK7HVJ7mmtPWU9XgfAuFTV\n85I8L8n/zOBCtadksF7w1a21753cyAA2pqma9jC8y88tSZ66eLvP4QURW1tru4/zNVuHX7Ontfbu\n4bbzk1yX5ILW2jXLfR3ARlBV35rBmrhPTHJmkoNJ3pXk5xZPEgCwetN2wdviupWjV/q2JE+vqkMZ\n3KnoAxn8o7+4z84Mzhq//+gXtPbpqlrIYFF58QtsWK21Tyb5rkmPA2CzmJr4rar7JHltko+01v5y\n5KGrMjjLcWOSx2QwNeK9VXVBa+2eDBafv3vJ1dhJcijJtvGPHACAjWJq4jeDJYUen8F8tqNaa28f\n+fRTVfXnGaxL+bSc/NI8i3dG+p4kN+XeywQBADB5WzJYD/3q1trfruUTT0X8VtXrMrhg7amtteXW\nvTyqtXZjVd2awVng+Qzmvz2gqs5ccvZ32/Cxpb4nyVvXZuQAAIzRs5O8bS2fcKLxW1WVwcLvz0zy\n9NbazSf4klTVNyZ5aAZ3VkoGd1v6agaLv49e8LY9yR8v8xQ3Jclb3vKWPO5xjzvNV8Dx7N27N/v2\n7Zv0MDYtx3f8HOPxcnzHzzEeL8d3vK677rpcdNFFybDb1tKkz/y+PsmeDOL3jqo6a7j971prR6rq\nwUkuTfLODObwPjqDq54/k8EdktJaO1xVb07ymqr6UpLbMwjqjx7nlqNHkuRxj3tcduzYMbYX1rut\nW7c6vmPk+I6fYzxeju/4Ocbj5fiumzWfojrp+H1BBqs5fHDJ9ucl+e0k/5DB/d6fk8FKEAcyiN6f\nb619dWT/vUnuyeDCuDMyuEju4jGOGwCADWii8dtaW/EOc621I0kuXMXz3JXkRcMPAABY1kRvbwwA\nAOtJ/DIWe/bsmfQQNjXHd/wc4/FyfMfPMR4vx3fjmqrbG6+HqtqR5Nprr73WRHUAgCm0f//+7Ny5\nM0l2ttb2r+VzO/MLAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3\nxC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQ\nDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAA\ndEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8A\nAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfEL\nAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8\nAgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0Q\nvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3\nxC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQ\nDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAAdEP8AgDQDfELAEA3xC8AAN0QvwAA\ndEP8AgDQDfELAEA3xC8AAN2YaPxW1c9W1cer6raqOlRVc1V13jL7vaqqDlTVV6rqD6vqMUse31JV\nr6+qW6vq9qp6Z1U9fP1eCQAAG8Gkz/w+NcnlSZ6c5LuS3D/J+6rqQYs7VNXLkrw4yU8M97sjydVV\ndcbI8+xL8v1JfjjJ05LMJHn3erwAAAA2jvtN8pu31r539POqel6SW5LsSPKRqqokL0ny6tbae4b7\nPCfJoSQ/mOTtVbU1yY8n2dNa++Bwnx9Lcl1VPbm1ds06vRwAAKbcpM/8LvWQ4Z9fGv75qCTbkrx/\ncYfW2m1JrklywXDTzgzOGI/u8+kkCyP7AADA9MRvVd0nyWuTfKS19pfDzWcN/zy0ZPdDGUTx4j53\nD6P4ePsAAMBkpz0s8fokj0/ylFXsW2MeCwAAm9BUxG9VvS7J9yV5amvtwMhDB4d/bsuxZ3+3Jdk/\nss8DqurMJWd/t418/b3s3bs3W7duPWbbnj17smfPnlN7EQAAnLTZ2dnMzs4es+3w4cNj+37VWhvb\nk5/wmw8uaLs8yTOTPL219tllHv98kstaa68ZbjszgxB+bmvtHcML3m7J4IK3dw/3OT/JdUm+rbX2\nJ0uec0eSa6+99trs2LFjvC8QAICTtn///uzcuTNJdrbW9p9o/5Mx6TO/r0+yJ4P4vaOqFuf4/l1r\n7UhrrVXVa5P8XFV9JslNSV6dQRD/bpK01g5X1ZuTvKaqvpTk9gyC+qNLwxcAgL5NOn5fkKQl+eCS\n7c9L8ttJ0lr7lap6cJIrMlgN4sNJLmyt3T2y/94k9yR5V5IzklyV5OJxDhwAgI1n0uv8rmq1idba\nJUkuWeHxu5K8aPgBAADLmpqlzgAAYNzELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wC\nANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/\nAAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfE\nLwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN\n8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0\nQ/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA\n3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsA\nQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wC\nANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3bjfpAcAAJvB7OxsZmdnkyRHjhzJzTff\nnLPPPjtbtmxJkuzZsyd79uyZ5BCBiF+AqSOiNqbR/13279+fnTt3ZnZ2Njt27JjwyIBR4hdgyogo\ngPEx5xcAgG6IXwAAumHaAwAb0ujc6BtvvDE33nhjHvjAByZJ7rzzzpx77rk555xzkpgnDfwj8QvA\nhrTc3OgPfehDSZKdO3fmyiuvNE8auBfxCwBMHaueMC7iF4CJEDesxKonjIv4BWAixA0wCVZ7AACg\nG878AjA1rrrqqlx66aVJTIUAxkP8AjA1LrzwwrziFa9IYioEMB7iF4AkLkAD+iB+AUjiAjSgD+IX\nYJNw5hbgxMQvwCbhzC3AiVnqDACAbjjzC8DU2Lt3b7Zu3ZojR47k05/+9DHbNsq0jV27duWGG25I\nkuzevTvnnntu5ufnJzwqYJH4BWBq7Nu3Lzt27Dg6bWN020Zx4MCBLCwsJEkWFhaOzrkGpoNpDwAA\ndMOZXwBOyEoSwGYhfgE4IStJrN7MzEyOHDmShYWFbN++PTMzM5MeEjBi4tMequqpVfWeqvp8Vd1T\nVc9c8viVw+2jH3+wZJ8tVfX6qrq1qm6vqndW1cPX95UAMAm7du3K7t27kwwuMHv+858/0fHMz89n\nbm4uSTI3N+diN5gyE4/fJA9K8skkLxx+3pY83pK8N8lZIx9Lf7e2L8n3J/nhJE9LMpPk3WMaLwBT\nZOkFZl/84hcnPCJgmk182kNr7aokVyVJVS23SyW5u7V2y7IPVm1N8uNJ9rTWPjjc9mNJrquqJ7fW\nrhnHuAEA2Him4czvibQkT6+qQ1V1fVW9oaq+fuTxnUnun+T9R7+gtU8nWUhywfoOFQCAaTbxM7+r\ncFWSdyW5McljkvxSkvdW1QWttXsymAZxd2vttiVfdyjJtnUdKUCHJr0SxOgFZmeccUYOHTqUxA0m\ngOVNffy21t4+8umnqurPk3w2g7m9/kUDmLBJrwQxPz9/9Ptu27ZtVTeYmHSwA5Mz9fG7VGvtxqq6\nNYOzwPNJDiZ5QFWdueTs77bhY8tavF3mKP/YAdPEbXLHZ9LBzur572DzG/1hdNHhw4fH9v02XPxW\n1TcmeWiSLww3XZvkq0mekeEKD1V1fpLtSf74eM+z0W6XCfSn59vkji5XJnj61vN/B71Y7uTj6C3O\n19rE47eqHpzksSObzq2qJyb52yRfSnJpkndmMIf30Ul+JclnklydJK21w1X15iSvqaovJbk9yeVJ\nPtpa+5P1eh0ArJ3R5cpOJnge9rCHHf0aN5gAljPx+E3ypCQfGP69JXnN8O9XJrk4yROSPCfJQ5Ic\nyCB6f7619tWR59ib5J4MLow7I4OL5C4e98ABmC5XXHFFkmTnzp2Zm5vzGz7gXiYev8O1eVdacu3C\nVTzHXUleNPwA2BR6uU3ucnM6YVEv/x2wfjbCOr8AXerlNrlL53QeOHDg6PSFJIKnc738d8D6mfiZ\nXwA2ttErtW+5ZXAzzosvvjgPf/jDk5zaSjpXXHHF0YtdTF8A1pL4BeAYJ7u01GjcvvWtb80111yT\nF7/4xXn2s5+9LuMFOBniF4BjrLS01DjWXDWnE1hP4heAVRvHmqujd2ibm5vLT//0T2f37t1HH3/+\n85+fT3ziE6f9fTYqd6ODtSV+AZgqo4GdHLvmb4/cjQ7WltUeADjGzMxMtm/fnsRKC8Dm48wvAMdY\nOg1h9Ayj+bnHt3R6wnnnnZeXv/zlpifAlBG/AJvMOC5KW7RSGK+V0cBOksOHD+e7v/u7p36u67SN\nB1ie+AXYZMZxUdp6Gg3sJPnABz6QJOa6AmvCnF8AALrhzC8AG9Lx5tjeddddSZKrrrrKWWLgXsQv\nwCYzqYvSdu3aleuvvz5J8tKXvjRvetOb1myu8XKON8d2ccrEhRdeOLbvDWxc4hdgg1t6BvTAgQN5\nxCMekYWFhZxzzjl5/vOfvy7jOHDgQA4ePJgkOXjwYM4888wTfs3i2A8fPpxkcIHe+eeff/TsLcBa\nE78AG9xyN0F4y1vekmuuuSb79u2b6l/9L459cdwLCwuZm5tLkqMXvI3TOFfGAKaT+AWgWxt9ZQzg\n5FntAYA1MTMzk7POOitJctZZZ7kBBjCVxC8Aa2J+fj6XXXZZkuSyyy4zfQCYSuIXgG7NzMxk+/bt\nSeJ2zdAJc36BbixdFWHab5fL6h1vzd8T/W877ts1e8/B9BG/QDeWWxXB7XKnz969e4/+ffv27au6\nccW0RqT3HEwf8Quwwax0NnFxvdyNbN++fUlyzNlYN67oz6mezYcTEb8AG8xKZxMXP4eNTtwyLuIX\ngBNyFm7y3JAD1ob4BeCExO3kuSEHrA1LnQEA0A1nfoGjLMsEwGYnfoGjLMtEMvgh6I1vfGOS5OKL\nL86Xv/zlVf0QtGvXrlx//fVJkpe+9KV505vedEpzUvfu3ZszzjjjmHnFm2EVi9M1MzOTI0eOZGFh\nwQ054DSIX4ApM+mLy/bs2ZPzzz8/O3fuzItf/OJcdNFFq/oh6MCBAzl48GCS5ODBgznzzDNX3H/p\n69y+ffvROa1btmzJpZdeeq8fxno27htyQC/EL8CUmdT0kuWi+/LLL08yOBv7ghe8YE3HtfR1Lobd\nvn37hB0wNuIXgCTLR/c0BKklvoC1JH4BWBMzMzO57bbbcvDgwZx11llrNifVEl/AWhK/AKtgJYwT\nm5+fz1vf+tZcdNFFueyyy/LsZz970kMCuBfxC7AKG2EljNHpAS996UsnPBqA6SR+ATaJ0ekBi6su\nrIfRs+K33HJLkuTyyy/P29/+9iSrOyu+0rxeS3wBa0n8AnBaljsr/oY3vOGkzoqvNK/XEl/AWhK/\nwIZgzi0Aa0H8AhvCRphzO2mj0wPOOuusdZ36sNFM+kYiwOSIX6A7m3Xd2NHpAZdddlkuuuiiSQ9p\n1dZ7Xq+4hX7dZ9IDAFhvS+eXHjhwYMIjOjW7du3K7t27kwwifteuXRMe0ambn5/P3NxckmRubm5T\n/DACTCdnfgE2qKURf/fdd2fv3r1JBqst+FU+wL2JX4BN4swzz8y+fftOabUFxmuzTrWBjUj8At2x\nbizrzS2aYXqY8wt0Z7PML52Zmcn27duTRMQDrJIzv8CmtdLawIcPH57w6E7fcjd/2L9//6SHBTDV\nxC9wL5tlfuJKawMvfg7rwVQbmB7iF7gX8xNhbZ3OLZrdkAPWlvgFgCkmbmFtueANAIBuiF/gXqwi\nAMBmJX6Be9ksS4EBwFLiFwCAbrjgDYBlbZYl7wBGiV8AlmXJO2AzOun4rar/neRzSf4oyYdaa59a\n81EBHIezkQCcjlOZ87s7ye8leWKSd1XVrVX17qr6f6vq/ms7PIBjLT0beeDAgQmPCICN5KTP/LbW\nbkkyO/xIVT06yWVJ9iR5UVV9V2vti2s6SgDW3aRvyevOZsA4nMq0h/8ryTlJ/mdr7c7W2mer6u2t\ntd+pqu9I8rNJfmqNxwnACvbu3ZutW7euaSSu9pa844pUcQuMw6lc8PbCJA9M8oaq+qMkn03yqCS/\n01r7cFU9ai0HCDBq0mcjp9W+ffuOG6fjJlKBjeRU4vfjSd6R5O4k35fkEUl+I0mq6gtJXrdmowM2\nnKVnAW+++eacffbZa/ar6tWejRwXF9wBbGynEr//JckPJvlAa+13ljz2jCTm+0LHRuN2MVJnZ2cn\ndlZyrU3D8l+baS7sZnotwMZwKhe8tSRzx3nMsmfA1Bk9W/uUpzwlD3rQg7Jjx44NG1obZZyrsZle\nC7AxuMkFsOmNnq29884788hHPjLve9/7JjwqACbhVNb5BejWzMxMtm/fniQuuAPYgMQvwEmYn5/P\n3Nxg5tfc3JyL3QA2GPELbHrO1gKwyJxfYFlXXXVVksHNE84444w1X7JsPU16eTT6ZTULmD7iF1jW\nhRdemFe+8pXZt29fkmy6JctgPYhbmD7iFzjqeGep7rrrriSDs8HiF4CNTPwCRx3vLNXilIELL7xw\nAqMCgLXjgjcAALohfgEA6Ib4BQCgG+IXAIBuiF8AALohfgEA6IalzoANwZ2yAFgL4hcYi127duWG\nG25IkuzevTvnnntu5ufnT/n5xC0Aa0H8AmNx4MCBLCwsJEkWFhaOnqEFgEky5xcAgG6IXwAAumHa\nA0yRpRd13XzzzTn77LM35EVdMzMzOXLkSBYWFrJ9+/bMzMxMekiskosLgc1M/MIUGY2K/fv3Z+fO\nnZmdnc2OHTsmPLKTNz8/f/Q1zM3NTeQ1iLhT47gAm5n4BTYtEQfAUuIXWNFaL1kGAJMkfoEVWbIM\ngM3Eag8AAHRD/AIA0A3xC6xoZmYm27dvTxJLlgGw4YlfYEXz8/OZm5tLkszNzbnYDYANzQVvAKtg\nzWCAzUH8AqyCuAXYHEx7AACgG+IXAIBuiF8AALohfgEA6Ib4BQCgG+IXAIBuiF8AALphnV+YQrt2\n7coNN9yQJNm9e3fOPfdcd1YDgDUgfmEKHThwIAsLC0mShYWFo3cR2wjcCQ2AaSZ+gTUlbgGYZub8\nAgDQDfELU2hmZibbt29Pkmzfvj0zMzMTHhEAbA7iF6bQ/Px85ubmkiRzc3MudgOANWLOLxve0gus\nbr755px99tkusAIA7kX8suGNxu3+/fuzc+fOzM7OZseOHRMeGQAwbcQvsCxLlgGwGYlfYFniFoDN\nyAVvAAB0Q/wCANAN8QsAQDfELwAA3Zh4/FbVU6vqPVX1+aq6p6qeucw+r6qqA1X1lar6w6p6zJLH\nt1TV66vq1qq6vareWVUPX79XAQDARjDx+E3yoCSfTPLC4edt9MGqelmSFyf5iSRPTnJHkqur6oyR\n3fYl+f4kP5zkaUlmkrx7vMMGAGCjmfhSZ621q5JclSRVdcxjNdjwkiSvbq29Z7jtOUkOJfnBJG+v\nqq1JfjzJntbaB4f7/FiS66rqya21a9bppQAAMOWm4czvSh6VZFuS9y9uaK3dluSaJBcMN+1Mcv8l\n+3w6ycLIPgAAMPXxe9bwz0NLth/KIIoX97l7GMXH2wcAAKY+fo+nTrwLAAAca+Jzfk/g4PDPbTn2\n7O+2JPtH9nlAVZ255OzvtpGvv5e9e/dm69atx2xzO1cAgPU1Ozub2dnZY7YdPnx4bN9v2uP3xgwC\n9hlJ/jxJqurMJP88yeuH+1yb5KvDfd493Of8JNuT/PHxnnjfvn3ZsWPH2AYOAMCJLXfycf/+/dm5\nc+dYvt/E47eqHpzksSObzq2qJyb529ba56rqtUl+rqo+k+SmJK9O8vkkv5skrbXDVfXmJK+pqi8l\nuT3J5Uk+2lr7k3V8KXDaRn/6PXLkSM4777y8/OUvz5YtW5L47QQAnK6Jx2+SJyX5wPDvLclrhn+/\nMsmPt9Z+ZRjIVyR5SJIPJ7mwtXb3yHPsTXJPknclOSODpdMuHv/QYW2JWwAYr4nH73Bt3hUvvGut\nXZLkkhXn8s4hAAAPZUlEQVQevyvJi4YfAACwrInHL6yVXbt25YYbbkiS7N69O+eee27m5+cnPCoA\nYJqIXzaNAwcOZGFhIUmysLBwdJ4sAMCijbrOLwAAnDTxCwBAN8Qvm8bMzEy2b9+eJNm+fXtmZmYm\nPCIAYNqIXzaN+fn5zM3NJUnm5uZc7AYA3Iv4BQCgG+IXAIBuiF8AALohfgEA6Ib4BQCgG+IXAIBu\niF8AALohfgEA6Ib4BQCgG+IXAIBuiF8AALohfgEA6Ib4BQCgG+IXAIBuiF8AALohfgEA6Ib4BQCg\nG+IXAIBu3G/SA4DTNTs7m9nZ2STJkSNHct555+XlL395tmzZkiTZs2dP9uzZM8khAgBTQvyy4Ylb\nAGC1THsAAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6I\nXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb\n4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDo\nhvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAA\nuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcA\ngG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgF\nAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+\nAQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6IXwAAuiF+AQDohvgFAKAb4hcAgG6I\nXwAAujH18VtVl1bVPUs+/nLJPq+qqgNV9ZWq+sOqesykxgsAwPSa+vgd+oskZ418PGXxgap6WZIX\nJ/mJJE9OckeSq6vqjAmMEwCAKXa/SQ9glf6htXbL0o1VVUlekuTVrbX3DLc9J8mhJD+Y5O3rOkoA\nAKbaRjnz+9iq+nxVfbaq3lJVjxxuf1SSbUnev7hja+22JNckuWAC4wQAYIpthPj9WJLnJvmeJD+Z\nQfB+uKr+SQZTIJLBmd5Rh0YeAwCAJBtg2kNr7aqRT/+iqq5JcnOSZyW5/jhfVknuGffYAADYWKY+\nfpdqrR2uqr9K8ugk88PN23Ls2d9tSfav9Dx79+7N1q1bj9m2Z8+e7NmzZw1HCwDASmZnZzM7O3vM\ntsOHD4/t+1VrbWxPPg7D6Q6fS/LzrbXXVdWBJJe11l4zfPzMDEL4ua21dyzz9TuSXHvttddmx44d\n6zl0AABWYf/+/dm5c2eS7GytrXhC82RN/Znfqrosye8nWUgyk+Q/Jrk7yeKPCK9N8nNV9ZkkNyV5\ndZLPJ/nddR8sAABTberjN8n/kUHoPjTJF5N8OMm3tdb+Nklaa79SVQ9OckWShwwfv7C1dveExgsA\nwJSa+vhtrZ1wEm5r7ZIkl6zDcAAA2MA2wlJnAACwJsQvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADd\nEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBA\nN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA\n0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8A\nAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8Qv\nAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3x\nCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD\n/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADd\nEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBA\nN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA\n0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBAN8QvAADdEL8AAHRD/AIA0A3xCwBANzZV/FbVC6vq\npqq6s6o+VlVPmvSYejU7OzvpIWxqju/4Ocbj5fiOn2M8Xo7vxrVp4reqfjTJryW5JMm3JvmzJFdX\n1cMmOrBO+UdhvBzf8XOMx8vxHT/HeLwc341r08Rvkp9KckVr7bdaa9cneUGSryT58ckOCwCAabEp\n4reqHpBkR5L3L25rrbXh5xdMalwAAEyXTRG/Sb4hyX2THFqy/ZYkZ63/cAAAmEb3m/QAJmBLklx3\n3XWTHsemdvjw4ezfv3/Sw9i0HN/xc4zHy/EdP8d4vBzf8RrptC1r/dw1mB2wsQ2nPdyR5Idaa78/\nsv23kpzZWts9su3fJHnr+o8SAICT9OzW2tvW8gk3xZnf1trdVXVtkmck+f0kqar7JPnOJL++ZPer\nkzw7yU1JjqzjMAEAWJ0tSc7JoNvW1KY485skVfWsJL+V5CeSfDzJS5L8cJJvaq19cZJjAwBgOmyK\nM79J0lp7x3BN31dlcJHbJ5NcKHwBAFi0ac78AgDAiWyWpc4AAOCExC8AAN3oLn6r6oVVdVNV3VlV\nH6uqJ016TBtRVV1aVfcs+fjLJfu8qqoOVNVXquoPq+oxkxrvtKuqp1bVe6rq88Nj+cxl9lnxeFbV\nlqp6fVXdWlW3V9U7q+rh6/cqptuJjnFVXbnMe/oPluzjGB9HVf1sVX28qm6rqkNVNVdV5y2zn/fx\nKVrNMfY+PnVV9ZNV9WdVdXj48dGqunDJPt6/p+FEx3i93r9dxW9V/WiSX0tySZJvTfJnSa4eXijH\nyfuLDC4uXPx4yuIDVfWyJC/OYPWNJ2ewDvPVVXXGBMa5ETwog4s0Xzj8/JjJ+Ks8nvuSfH8Gq5w8\nLclMknePd9gbyorHePj5e3Pse3rPkn0c4+N7apLLM3h/fleS+yd5X1U9aHEH7+PTdsJjHO/j0/G5\nJC9LsiPJziQfSPL7VfXNiffvGlnxGGe93r+ttW4+klyT5NdHPq8kf5PkZZMe20b7SHJpkk8e57FK\n8oUkPzWy7cwkdyb50UmPfdo/ktyT5F+ezPFMsjXJXUn+1cg+5w+f68mTfk3T9rH0GA+3XZlkboWv\ncYxP7hh/w/DYPGX4uffxmI/xcJv38doe479N8mPev+M/xsO/r8v7t5szvzW4C9yOJO9f3NYGR+39\nSS6Y1Lg2uMcOf4X82ap6S1U9crj9UUm25dhjfVsGP3w41idvNcdzZwZngUb3+XSShTjmq9WSPH34\n6+Trq+oNVfX1I487xifnIcM/vzT80/t47S09xon38ZqoqvtW1b9OckaSD8f7d80tc4yTdXr/bpp1\nflfhG5LcN8mhJdtvSfJN6z+cDe9jSZ6b5NMZ/MrhkiQfrqp/lsGvKZJ7H+tDI4+xeisdz20j+9w9\n/Mf4ePuwsquSvCvJjUkek+SXkry3qi5ord0Tx3jVanCHzdcm+UhrbfFaAO/jNXScY5x4H5+WqnpC\nkj/OIMjuTPKs1tpfV9W3D3fx/j1NxzvGw4fX5f3bU/yyhlprV418+hdVdU2Sm5M8K8n1x/myyuBX\nE6yNmvQANpPW2ttHPv1UVf15ks9mMKdsfjKj2rBen+TxGbkOYAXex6dm2WPsfXzark/yLRn8ev1H\nkvxOVT19hf29f0/esse4tbZ/vd6/3Ux7SHJrkn/IvX8y2JbBPB5OQ2vtcJK/SvLo/OPxXO5YH1zP\ncW0Si8dspeN5MMkDqurMFfbhJLTWbszg343Fq7kd41Woqtcl+b4ku1prB0Ye8j5eIysc43vxPj45\nrbWvttZuaK19srX2igymNfxkVvf/a47tKqxwjJfbdyzv327it7V2d5Jrkzxjcdvw10bfmcHpd05D\nVf2TJI9N8oXhm/Vgjj3WZyb553GsT8Vqjue1Sb66ZJ/zk2yPY35Kquobkzw0//h/eo7xCmrgdUme\nmeRftNZuXrKL9/FpWsUxXu5rvI9Pz32T3GeV/7/m2J6a++Y4PTq29++kr/Jb5ysKn5XB/JLnJHlc\nkt/I4CrDh016bBvtI8llGSy7c06Sb0/yhxnMuXno8PGfyeAijB9I8oQkv5vkr5M8YNJjn8aPJA9O\n8sThxz1JXjL8+yNXezyTvCHJTUmensFFAR/NYD7gxF/fNHysdIyHj/1qBssXnZPBD8XXZvDrufs7\nxqs6vm9I8uXhvwujyxRtGdnH+3iMx9j7+LSP7y8n+Y7hsXvC8POvZfCDhvfvmI/xer5/J34gJnDg\nXzg8aEcy+CnhSZMe00b8SDKb5PPD4/i5JG9L8qgl+/zHDH5auzPJ+5I8ZtLjntaP4X/E9ww//mHk\n7/9ttcczg4sHXpfBD3R/n+SdSR4+6dc2LR8rHeMkWzK40OJQBsvo3JjkjVnyg7FjvOLxXXpcFz+e\ns2Q/7+MxHWPv49M+vm8aHrMjw2P4viTfuWQf798xHeP1fP/W8IkAAGDT62bOLwAAiF8AALohfgEA\n6Ib4BQCgG+IXAIBuiF8AALohfgEA6Ib4BQCgG+IXAIBuiF8AALohfgEA6Ib4BQCgG+IXAIBu3G/S\nAwDg1FTVtyX5piTfmuR/JdmW5AeS/LvW2qFJjg1gWolfgA2oqr4uyWNba1dW1R1J9ib5zuHHnRMd\nHMAUq9bapMcAwEmqqgcm+Wpr7WtV9atJPtda+/VJjwtg2pnzC7ABtdbubK19bfjpMzKY9pCqOnNy\nowKYfuIXYAOqqh+oqpdU1aMzmP7wqaq6T5LnTHpsANPMtAeADaiqnpdkR5LrkjwkyR1Jvpbkba21\nv5vg0ACmmvgFAKAbpj0AANAN8QsAQDfELwAA3RC/AAB0Q/wCANAN8QsAQDfELwAA3RC/AAB0Q/wC\nANAN8QsAQDfELwAA3RC/AAB04/8H5uEG9rCOIlIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1041aac10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x,y,sigmay) = generate_data()\n",
    "\n",
    "plot_yerr(x, y, sigmay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares fitting\n",
    "\n",
    "* An industry standard: find the slope and intercept that minimize the mean square residual. Since the data depend linearly on the parameters, the least squares solution can be found by a matrix inversion and multiplication, conveneniently packed in `numpy.linalg`.\n",
    "\n",
    "\n",
    "* Suppose that the $y$ values are related to the $x$ values by $y = m x + b$, where $x$ and $y$ are \"vectors\" (`numpy` arrays). We want to find $m$ and $b$ that minimize\n",
    "\n",
    "$\\chi^2 = \\sum_k \\frac{(y_k - m x_k - b)^2}{\\sigma_k^2}$\n",
    "\n",
    "\n",
    "* Here, $1/\\sigma_k^2$ is a sensible weighting factor that reduces the influence of $y_k$ values with large uncertainty $\\sigma_k$\n",
    "\n",
    "\n",
    "* Differentiating $\\chi^2$ with respect to $m$ and $b$ and setting the result to zero leads to a matrix equation that can be solved - this is what `np.linalg.lstsq` is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares (maximum likelihood) estimator: 89.3641005525 0.250919464805\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAK9CAYAAAAt0QTlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noA3xCwBAG+IXAIA2xC8AAG38P/IqkZIR5OvLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1067b5890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear algebra: weighted least squares\n",
    "N = len(x)\n",
    "A = np.zeros((N,2))\n",
    "A[:,0] = 1. / sigmay\n",
    "A[:,1] =  x / sigmay\n",
    "b = y / sigmay\n",
    "theta,nil,nil,nil = np.linalg.lstsq(A, b)\n",
    "plot_yerr(x, y, sigmay)\n",
    "b_ls,m_ls = theta\n",
    "print('Least Squares (maximum likelihood) estimator:', b_ls,m_ls)\n",
    "plot_line(m_ls, b_ls);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Fitting\n",
    "\n",
    "* Ref to this morning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating posterior probability on a grid\n",
    "\n",
    "This procedure will get us the Bayesian solution to the problem - not an estimator, but a probability distribution for the parameters m and b. This PDF captures the uncertainty in the model parameters given the data. For simple, 2-dimensional parameter spaces like this one, evaluating on a grid is not a bad way to go. We'll see that the least squares solution lies at the peak of the posterior PDF - for a certain set of assumptions about the data and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def straight_line_log_likelihood(x, y, sigmay, m, b):\n",
    "    '''\n",
    "    Returns the log-likelihood of drawing data values *y* at\n",
    "    known values *x* given Gaussian measurement noise with standard\n",
    "    deviation with known *sigmay*, where the \"true\" y values are\n",
    "    *y_t = m * x + b*\n",
    "\n",
    "    x: list of x coordinates\n",
    "    y: list of y coordinates\n",
    "    sigmay: list of y uncertainties\n",
    "    m: scalar slope\n",
    "    b: scalar line intercept\n",
    "\n",
    "    Returns: scalar log likelihood\n",
    "    '''\n",
    "    return (np.sum(np.log(1./(np.sqrt(2.*np.pi) * sigmay))) +\n",
    "            np.sum(-0.5 * (y - (m*x + b))**2 / sigmay**2))\n",
    "\n",
    "\n",
    "def straight_line_log_prior(m, b, mlimits, blimits):\n",
    "    # Uniform in m:\n",
    "    if (m < mlimits[0]) | (m > mlimits[1]):\n",
    "        log_m_prior = -np.inf\n",
    "    else:\n",
    "        log_m_prior = -np.log(mlimits[1] - mlimits[0])\n",
    "    # Uniform in b:\n",
    "    if (b < blimits[0]) | (b > blimits[1]):\n",
    "        log_b_prior = -np.inf\n",
    "    else:\n",
    "        log_b_prior = -np.log(blimits[1] - blimits[0])\n",
    "        \n",
    "    return log_m_prior + log_b_prior\n",
    "\n",
    "\n",
    "def straight_line_log_posterior(x,y,sigmay,m,b,mlimits,blimits):\n",
    "    return (straight_line_log_likelihood(x,y,sigmay,m,b) +\n",
    "            straight_line_log_prior(m,b,mlimits,blimits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate log P(m,b | x,y,sigmay) on a grid.\n",
    "\n",
    "# Define uniform prior limits, enforcing positivity in both parameters:\n",
    "mlimits = [0.0, 2.0]\n",
    "blimits = [0.0, 200.0]\n",
    "\n",
    "# Set up grid:\n",
    "mgrid = np.linspace(mlimits[0], mlimits[1], 101)\n",
    "bgrid = np.linspace(blimits[0], blimits[1], 101)\n",
    "log_posterior = np.zeros((len(mgrid),len(bgrid)))\n",
    "\n",
    "# Evaluate log posterior PDF:\n",
    "for im,m in enumerate(mgrid):\n",
    "    for ib,b in enumerate(bgrid):\n",
    "        log_posterior[im,ib] = straight_line_log_posterior(x, y, sigmay, m, b, mlimits, blimits)\n",
    "\n",
    "# Convert to probability density and plot\n",
    "\n",
    "posterior = np.exp(log_posterior - log_posterior.max())\n",
    "\n",
    "plt.imshow(posterior, extent=[blimits[0],blimits[1],mlimits[0],mlimits[1]],cmap='Blues',\n",
    "           interpolation='none', origin='lower', aspect=(blimits[1]-blimits[0])/(mlimits[1]-mlimits[0]),\n",
    "           vmin=0, vmax=1)\n",
    "plt.contour(bgrid, mgrid, posterior, pdf_contour_levels(posterior), colors='k')\n",
    "\n",
    "i = np.argmax(posterior)\n",
    "i,j = np.unravel_index(i, posterior.shape)\n",
    "print('Grid maximum posterior values (m,b) =', mgrid[i], bgrid[j])\n",
    "\n",
    "plt.title('Straight line: posterior PDF for parameters');\n",
    "plt.plot(b_ls, m_ls, 'r+', ms=12, mew=4);\n",
    "plot_mb_setup(mlimits,blimits);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC Sampling\n",
    "\n",
    "In problems with higher dimensional parameter spaces, we need a more efficient way of approximating the posterior PDF - both when characterizing it in the first place, and then when doing integrals over that PDF (to get the marginalized PDFs for the parameters, or to compress them in to single numbers with uncertainties that can be easily reported). In most applications it's sufficient to approximate a PDF with a (relatively) small number of samples drawn from it; MCMC is a procedure for drawing samples from PDFs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def straight_line_posterior(x, y, sigmay, m, b, mlimits, blimits):\n",
    "    return np.exp(straight_line_log_posterior(x, y, sigmay, m, b, mlimits, blimits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial m, b, at center of ranges\n",
    "m,b = 0.5*(mlimits[0]+mlimits[1]), 0.5*(blimits[0]+blimits[1])\n",
    "\n",
    "# step sizes, 5% of the prior\n",
    "mstep, bstep = 0.05*(mlimits[1]-mlimits[0]), 0.1*(blimits[1]-blimits[0])\n",
    "        \n",
    "# how many steps?\n",
    "nsteps = 10000\n",
    "    \n",
    "chain = []\n",
    "probs = []\n",
    "naccept = 0\n",
    "    \n",
    "print('Running MH for', nsteps, 'steps')\n",
    "\n",
    "# First point:\n",
    "L_old    = straight_line_log_likelihood(x, y, sigmay, m, b)\n",
    "p_old    = straight_line_log_prior(m, b, mlimits, blimits)\n",
    "logprob_old = L_old + p_old\n",
    "\n",
    "for i in range(nsteps):\n",
    "    # step\n",
    "    mnew = m + np.random.normal() * mstep\n",
    "    bnew = b + np.random.normal() * bstep\n",
    "\n",
    "    # evaluate probabilities\n",
    "    L_new    = straight_line_log_likelihood(x, y, sigmay, mnew, bnew)\n",
    "    p_new    = straight_line_log_prior(mnew, bnew, mlimits, blimits)\n",
    "    logprob_new = L_new + p_new\n",
    "\n",
    "    if (np.exp(logprob_new - logprob_old) > np.random.uniform()):\n",
    "        # Accept the new sample:\n",
    "        m = mnew\n",
    "        b = bnew\n",
    "        L_old = L_new\n",
    "        p_old = p_new\n",
    "        logprob_old = logprob_new\n",
    "        naccept += 1\n",
    "    else:\n",
    "        # Stay where we are; m,b stay the same, and we append them\n",
    "        # to the chain below.\n",
    "        pass\n",
    "\n",
    "    chain.append((b,m))\n",
    "    probs.append((L_old,p_old))\n",
    "    \n",
    "print('Acceptance fraction:', naccept/float(nsteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull m and b arrays out of the Markov chain and plot them:\n",
    "mm = [m for b,m in chain]\n",
    "bb = [b for b,m in chain]\n",
    "\n",
    "# Traces, for convergence inspection:\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(mm, 'k-')\n",
    "plt.ylim(mlimits)\n",
    "plt.ylabel('m')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(bb, 'k-')\n",
    "plt.ylabel('Intercept b')\n",
    "plt.ylim(blimits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This looks pretty good: no plateauing, or drift.\n",
    "\n",
    "\n",
    "* A more rigorous test for convergence is due to Gelman & Rubin, and involves comparing the intrachain variance with the inter-chain variance in an ensemble. It's worth reading up on. \n",
    "\n",
    "\n",
    "* Foreman-Mackey & Hogg recommend looking at the autocorrelation length, and whther it stablizes during the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at samples in parameter space.\n",
    "\n",
    "# First show contours from gridding calculation:\n",
    "plt.contour(bgrid, mgrid, posterior, pdf_contour_levels(posterior), colors='k')\n",
    "plt.gca().set_aspect((blimits[1]-blimits[0])/(mlimits[1]-mlimits[0]))\n",
    "\n",
    "# Scatterplot of m,b posterior samples, overlaid:\n",
    "plt.plot(bb, mm, 'b.', alpha=0.1)\n",
    "plot_mb_setup(mlimits,blimits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 and 2D marginalised distributions:\n",
    "\n",
    "!pip install --upgrade --no-deps corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import corner\n",
    "corner.corner(chain, labels=['b','m'], range=[blimits,mlimits],quantiles=[0.16,0.5,0.84],\n",
    "                show_titles=True, title_args={\"fontsize\": 12},\n",
    "                plot_datapoints=True, fill_contours=True, levels=[0.68, 0.95], \n",
    "                color='b', bins=80, smooth=1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looks like a measurement - but let's do some checks first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checking\n",
    "\n",
    "How do we know if our model is any good? There are two properties that \"good\" models have: the first is accuracy, and the second is efficiency. Accurate models generate data that is *like* the observed data. What does this mean? First we have to define what similarity is, in this context. Visual impression is one very important way. Test statistics that capture relevant features of the data are another. Let's look at the posterior predictive distributions for the datapoints, and for a particularly interesting test statistic, the reduced chi-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Posterior visual check, in data space\n",
    "X = np.array(xlimits)\n",
    "for i in (np.random.rand(100)*len(chain)).astype(int):\n",
    "    b,m = chain[i]\n",
    "    plt.plot(X, b+X*m, 'b-', alpha=0.1)\n",
    "plot_line(m_ls, b_ls);\n",
    "plot_yerr(x, y, sigmay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test statistics: functions of the data, not the parameters.\n",
    "\n",
    "# 1) Reduced chisq for the best fit model:\n",
    "def test_statistic(x,y,sigmay,b_ls,m_ls):\n",
    "  return np.sum((y - m_ls*x - b_ls)**2.0/sigmay**2.0)/(len(y)-2)\n",
    "\n",
    "# 2) Reduced chisq for the best fit m=0 model:\n",
    "# def test_statistic(x,y,sigmay,dummy1,dummy2):\n",
    "#    return np.sum((y - np.mean(y))**2.0/sigmay**2.0)/(len(y)-1)\n",
    "\n",
    "# 3) Weighted mean y:\n",
    "# def test_statistic(x,y,sigmay,dummy1,dummy2):\n",
    "#    return np.sum(y/sigmay**2.0)/np.sum(1.0/sigmay**2.0)\n",
    "\n",
    "# 4) Variance of y:\n",
    "# def test_statistic(x,y,sigmay,dummy1,dummy2):\n",
    "#    return np.var(y)\n",
    "\n",
    "\n",
    "# Approximate the posterior predictive distribution for T, \n",
    "# by drawing a replica dataset for each sample (m,b) and computing its T:\n",
    "T = np.zeros(len(chain))\n",
    "for k,(b,m) in enumerate(chain):\n",
    "    yrep = b + m*x + np.random.randn(len(x)) * sigmay\n",
    "    T[k] = test_statistic(x,yrep,sigmay,b_ls,m_ls)\n",
    "    \n",
    "# Compare with the test statistic of the data, on a plot:   \n",
    "Td = test_statistic(x,y, sigmay, b_ls, m_ls)\n",
    "\n",
    "plt.hist(T, 100, histtype='step', color='blue', lw=2, range=(0.0,np.percentile(T,99.0)))\n",
    "plt.axvline(Td, color='black', linestyle='--', lw=2)\n",
    "plt.xlabel('Test statistic')\n",
    "plt.ylabel('Posterior predictive distribution')\n",
    "\n",
    "\n",
    "# What is Pr(T>T(d)|d)?\n",
    "\n",
    "greater = (T > Td)\n",
    "P = 100*len(T[greater])/(1.0*len(T))\n",
    "print(\"Pr(T>T(d)|d) = \",P,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If our model is true (and we're just uncertain about its parameters, given the data), we can compute the probability of getting a $T$ less than that observed, where T is the reduced chisq relative to a straight line with some reference $(m,b)$.\n",
    "\n",
    "\n",
    "* Note that we did not have to look up the \"chi squared distribution\" - we can simply compute the posterior predictive distribution given our generative model.\n",
    "\n",
    "\n",
    "* Still, this test statistic looks a little bit strange: it's computed relative to the best fit straight line - which is a sensible reference but somehow not really in the spirit of fitting the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead, lets look at a *discrepancy* $T(d,\\theta)$ that is a function of both the data and the parameters, and compute the posterior probability of getting $T(d^{\\rm rep},\\theta) > T(d,\\theta)$\n",
    "\n",
    "${\\rm Pr}(T[d^{\\rm rep},\\theta] > T[d,\\theta]|d) = {\\rm Pr}(T[d^{\\rm rep},\\theta] > T[d,\\theta]|\\theta,d)\\;{\\rm Pr}(\\theta|d)\\;d\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discrepancy: functions of the data AND parameters.\n",
    "\n",
    "# 1) Reduced chisq for the model:\n",
    "def test_statistic(x,y,sigmay,b,m):\n",
    "   return np.sum((y - m*x - b)**2.0/sigmay**2.0)/(len(y)-2)\n",
    "\n",
    "# Approximate the posterior predictive distribution for T, \n",
    "# by drawing a replica dataset for each sample (m,b) and computing its T, \n",
    "# AND ALSO its Td:\n",
    "T = np.zeros(len(chain))\n",
    "Td = np.zeros(len(chain))\n",
    "for k,(b,m) in enumerate(chain):\n",
    "    yrep = b + m*x + np.random.randn(len(x)) * sigmay\n",
    "    T[k] = test_statistic(x,yrep,sigmay,b,m)\n",
    "    Td[k] = test_statistic(x,y,sigmay,b,m)\n",
    "    \n",
    "# Compare T with Td, on a scatter plot:   \n",
    "\n",
    "plt.scatter(Td, T, color='blue',alpha=0.1)\n",
    "plt.plot([0.0, 100.0], [0.0, 100.], color='k', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Observed discrepancy $T(d,\\\\theta)$')\n",
    "plt.ylabel('Replicated discrepancy $T(d^{\\\\rm rep},\\\\theta)$')\n",
    "plt.ylim([0.0,np.percentile(Td,99.0)])\n",
    "plt.xlim([0.0,np.percentile(Td,99.0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of differences:\n",
    "\n",
    "diff = T-Td\n",
    "plt.hist(diff, 100, histtype='step', color='blue', lw=2, range=(np.percentile(diff,1.0),np.percentile(diff,99.0)))\n",
    "plt.axvline(0.0, color='black', linestyle='--', lw=2)\n",
    "plt.xlabel('Difference $T(d^{\\\\rm rep},\\\\theta) - T(d,\\\\theta)$')\n",
    "plt.ylabel('Posterior predictive distribution')\n",
    "\n",
    "\n",
    "# What is Pr(T>T(d)|d)?\n",
    "\n",
    "greater = (T > Td)\n",
    "Pline = 100*len(T[greater])/(1.0*len(T))\n",
    "print(\"Pr(T>T(d)|d) = \",Pline,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The conclusion drawn from the discrepancy is more conservative. All our $\\theta = (m,b)$ samples are plausible, so replica datasets generated by them should also be plausible: the straight line defined by each $(m,b)$ should go through the real data points as readily (on average) as it does its replica dataset.\n",
    "\n",
    "\n",
    "* Do our posterior predictive $p-$values suggest we need to improve our model? What about the visual check?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher order polynomial?\n",
    "\n",
    "* Maybe I see some curvature in the data. Let's try adding an extra parameter to the model, to see if our data are better modeled using a quadratic function than a straight line. \n",
    "\n",
    "\n",
    "$y = m x + b + q*x**2$\n",
    "\n",
    "* The coefficient $q$ is probably pretty small (we were expecting to only have to use a straight line model for these data!), so we can assign a fairly narrow prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def quadratic_log_likelihood(x, y, sigmay, m, b, q):\n",
    "    '''\n",
    "    Returns the log-likelihood of drawing data values y at\n",
    "    known values x given Gaussian measurement noise with standard\n",
    "    deviation with known sigmay, where the \"true\" y values are\n",
    "    y_t = m*x + b + q**2\n",
    "\n",
    "    x: list of x coordinates\n",
    "    y: list of y coordinates\n",
    "    sigmay: list of y uncertainties\n",
    "    m: scalar slope\n",
    "    b: scalar line intercept\n",
    "    q: quadratic term coefficient\n",
    "\n",
    "    Returns: scalar log likelihood\n",
    "    '''\n",
    "    return (np.sum(np.log(1./(np.sqrt(2.*np.pi) * sigmay))) +\n",
    "            np.sum(-0.5 * (y - (m*x + b + q*x**2))**2 / sigmay**2))\n",
    "    \n",
    "    \n",
    "def quadratic_log_prior(m, b, q, mlimits, blimits, qpars):\n",
    "    # m and b:\n",
    "    log_mb_prior = straight_line_log_prior(m, b, mlimits, blimits)\n",
    "    # q:\n",
    "    log_q_prior = np.log(1./(np.sqrt(2.*np.pi) * qpars[1])) - \\\n",
    "                  0.5 * (q - qpars[0])**2 / qpars[1]**2\n",
    "    return log_mb_prior + log_q_prior\n",
    "    \n",
    "    \n",
    "def quadratic_log_posterior(x,y,sigmay,m,b,q):\n",
    "    return (quadratic_log_likelihood(x,y,sigmay,m,b,q) +\n",
    "            quadratic_log_prior(m,b,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign Gaussian prior for q:\n",
    "qpars = [0.0,0.003]\n",
    "\n",
    "# initial m, b, q, at center of ranges\n",
    "m,b,q = 0.5, 50, 0.0\n",
    "\n",
    "# step sizes\n",
    "mstep, bstep, qstep = 0.05, 5.0, 0.0003\n",
    "        \n",
    "# how many steps?\n",
    "nsteps = 10000\n",
    "    \n",
    "chain = []\n",
    "probs = []\n",
    "naccept = 0\n",
    "    \n",
    "print('Running MH for', nsteps, 'steps')\n",
    "\n",
    "# First point:\n",
    "L_old    = quadratic_log_likelihood(x, y, sigmay, m, b, q)\n",
    "p_old    = quadratic_log_prior(m, b, q, mlimits, blimits, qpars)\n",
    "logprob_old = L_old + p_old\n",
    "\n",
    "for i in range(nsteps):\n",
    "    # step\n",
    "    mnew = m + np.random.normal() * mstep\n",
    "    bnew = b + np.random.normal() * bstep\n",
    "    qnew = q + np.random.normal() * qstep\n",
    "\n",
    "    # evaluate probabilities\n",
    "    L_new    = quadratic_log_likelihood(x, y, sigmay, mnew, bnew, qnew)\n",
    "    p_new    = quadratic_log_prior(mnew, bnew, qnew, mlimits, blimits, qpars)\n",
    "    logprob_new = L_new + p_new\n",
    "\n",
    "    if (np.exp(logprob_new - logprob_old) > np.random.uniform()):\n",
    "        # Accept the new sample:\n",
    "        m = mnew\n",
    "        b = bnew\n",
    "        q = qnew\n",
    "        L_old = L_new\n",
    "        p_old = p_new\n",
    "        logprob_old = logprob_new\n",
    "        naccept += 1\n",
    "    else:\n",
    "        # Stay where we are; m,b stay the same, and we append them\n",
    "        # to the chain below.\n",
    "        pass\n",
    "\n",
    "    chain.append((b,m,q))\n",
    "    probs.append((L_old,p_old))\n",
    "    \n",
    "print('Acceptance fraction:', naccept/float(nsteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull m, b and q arrays out of the Markov chain and plot them:\n",
    "mm = [m for b,m,q in chain]\n",
    "bb = [b for b,m,q in chain]\n",
    "qq = [q for b,m,q in chain]\n",
    "\n",
    "# Traces, for convergence inspection:\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(mm, 'k-')\n",
    "plt.ylim(mlimits)\n",
    "plt.ylabel('m')\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(bb, 'k-')\n",
    "plt.ylim(blimits)\n",
    "plt.ylabel('Intercept b')\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(qq, 'k-')\n",
    "plt.ylim([qpars[0]-3*qpars[1],qpars[0]+3*qpars[1]])\n",
    "plt.ylabel('Quadratic coefficient q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corner.corner(chain, labels=['b','m','q'], range=[blimits,mlimits,(qpars[0]-3*qpars[1],qpars[0]+3*qpars[1])],quantiles=[0.16,0.5,0.84],\n",
    "                show_titles=True, title_args={\"fontsize\": 12},\n",
    "                plot_datapoints=True, fill_contours=True, levels=[0.68, 0.95], \n",
    "                color='green', bins=80, smooth=1.0);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Quadratic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Posterior visual check, in data space\n",
    "X = np.linspace(xlimits[0],xlimits[1],100)\n",
    "for i in (np.random.rand(100)*len(chain)).astype(int):\n",
    "    b,m,q = chain[i]\n",
    "    plt.plot(X, b + X*m + q*X**2, 'g-', alpha=0.1)\n",
    "plot_line(m_ls, b_ls);\n",
    "plot_yerr(x, y, sigmay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discrepancy: functions of the data AND parameters.\n",
    "\n",
    "# 1) Reduced chisq for the model:\n",
    "def test_statistic(x,y,sigmay,b,m,q):\n",
    "   return np.sum((y - m*x - b - q*x**2)**2.0/sigmay**2.0)/(len(y)-3)\n",
    "\n",
    "# Approximate the posterior predictive distribution for T, \n",
    "# by drawing a replica dataset for each sample (m,b) and computing its T, \n",
    "# AND ALSO its Td:\n",
    "T = np.zeros(len(chain))\n",
    "Td = np.zeros(len(chain))\n",
    "for k,(b,m,q) in enumerate(chain):\n",
    "    yp = b + m*x + q*x**2 + sigmay*np.random.randn(len(x))\n",
    "    T[k] = test_statistic(x,yp,sigmay,b,m,q)\n",
    "    Td[k] = test_statistic(x,y,sigmay,b,m,q)\n",
    "\n",
    "# Histogram of differences:\n",
    "diff = T - Td\n",
    "plt.hist(diff, 100, histtype='step', color='green', lw=2, range=(np.percentile(diff,1.0),np.percentile(diff,99.0)))\n",
    "plt.axvline(0.0, color='black', linestyle='--', lw=2)\n",
    "plt.xlabel('Difference $T(d^{\\\\rm rep},\\\\theta) - T(d,\\\\theta)$')\n",
    "plt.ylabel('Posterior predictive distribution')\n",
    "\n",
    "# What is Pr(T>T(d)|d)?\n",
    "greater = (T > Td)\n",
    "Pquad = 100*len(T[greater])/(1.0*len(T))\n",
    "print(\"Pr(T>T(d)|d,quadratic) = \",Pquad,\"%, cf. Pr(T>T(d)|d,straightline) = \",Pline,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This model checks out too: both the quadratic and linear models provide reasonable fits to the data, with the posterior predictive replica datasets behaving acceptably similarly to the real data.\n",
    "\n",
    "\n",
    "* But, the predictions seem comparably precise! Which model should we prefer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison with the Bayesian Evidence\n",
    "\n",
    "* The evidence for model $H$, ${\\rm Pr}(d|H)$, enables a form of Bayesian hypothesis testing via the evidence ratio (or \"odds ratio\", or \"Bayes Factor\"):\n",
    "\n",
    "$R = \\frac{{\\rm Pr}(d|H_1)}{{\\rm Pr}(d|H_0)}$\n",
    "\n",
    "\n",
    "* This quantity is similar to a likelihood ratio, but its a *fully marginalized likelihood ratio* - which is to say that it *takes into account our uncertainty about values of the parameters of each model* by integrating over them all.\n",
    "\n",
    "\n",
    "* As well predictive accuracy, the other virtue a model can have is *efficiency*. The evidence *summarizes all the information we put into our model inferences*, via both the data and our prior beliefs.  You can see this by inspection of the integrand of the fully marginalized likelihood (#FML) integral:\n",
    "\n",
    "${\\rm Pr}(d|H) = \\int\\;{\\rm Pr}(d|\\theta,H)\\;{\\rm Pr}(\\theta|H)\\;d\\theta$\n",
    "\n",
    "\n",
    "* The following figure might help illustrate how the evidence depends on both goodness of fit (through the likelihood) and the complexity of the model (via the prior). In this 1D case, a Gaussian likelihood (red) is integrated over a uniform prior (blue): the evidence can be shown to be given by $E = f \\times L_{\\rm max}$, where $L_{\\rm max}$ is the maximum possible likelihood, and $f$ is the fraction of the blue dashed area that is shaded red. $f$ is 0.31, 0.98, and 0.07 in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('evidence.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The illustration above shows us a few things:\n",
    "\n",
    "1) The evidence can be made arbitrarily small by increasing the prior volume: the evidence is more conservative than focusing on the goodness of fit ($L_{\\rm max}$) alone - and if you assign a prior you don't believe, you should not expect to get out a meaningful evidence value.\n",
    "\n",
    "2) The evidence is linearly sensitive to prior volume ($f$), but exponentially sensitive to goodness of fit ($L_{\\rm max}$). It's still a likelihood, after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The odds ratio can, in principle, be combined with the ratio of priors for each model to give us the relative probability for each model being true, given the data:\n",
    "\n",
    "$\\frac{{\\rm Pr}(H_1|d)}{{\\rm Pr}(H_0|d)} = \\frac{{\\rm Pr}(d|H_1)}{{\\rm Pr}(d|H_0)} \\; \\frac{{\\rm Pr}(H_1)}{{\\rm Pr}(H_0)}$\n",
    "\n",
    "\n",
    "* Prior probabilities are very difficult to assign in most practical problems (notice that no theorist ever provides them). So, one way to interpret the evidence ratio is to note that:\n",
    "\n",
    "  * If you think that having seen the data, the two models are *still equally probable,*\n",
    "  * then the evidence ratio in favor of $H_1$ is you the odds that you would have had to have been willing to take against $H_1$, before seeing the data.\n",
    "  * That is: the evidence ratio updates the prior ratio into a posterior one - as usual.\n",
    "  \n",
    "  \n",
    "* The FML is in general quite difficult to calculate, since it involves averaging the likelihood over the prior. MCMC gives us samples from the posterior - and these cannot, it turns out, be reprocessed so as to estimate the evidence stably.\n",
    "\n",
    "\n",
    "* If we draw samples from the *prior*, however, we can then estimate the evidence via the usual sum over samples,\n",
    "\n",
    "${\\rm Pr}(d|H) \\approx \\sum_k\\;{\\rm Pr}(d|\\theta,H)$\n",
    "  \n",
    "\n",
    "* Sampling the prior and computing the likelihood at each sample position is called \"Simple Monte Carlo\", and while it works in certain low-dimensional situations, in general it is very inefficient (at best). Still, let's give it a try on our two models, and attempt to compute the Bayes Factor \n",
    "  \n",
    "$R = \\frac{{\\rm Pr}(d\\,|\\,{\\rm quadratic})}{{\\rm Pr}(d\\,|\\,{\\rm straight line})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw a large number of prior samples and hope for the best:\n",
    "N=50000\n",
    "mm = np.random.uniform(mlimits[0],mlimits[1], size=N)\n",
    "bb = np.random.uniform(blimits[0],blimits[1], size=N)\n",
    "qq = qpars[0] + qpars[1]*np.random.randn(N)\n",
    "\n",
    "log_likelihood_straight_line = np.zeros(N)\n",
    "log_likelihood_quadratic = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    log_likelihood_straight_line[i] = straight_line_log_likelihood(x, y, sigmay, mm[i], bb[i])\n",
    "    log_likelihood_quadratic[i] = quadratic_log_likelihood(x, y, sigmay, mm[i], bb[i], qq[i])\n",
    "\n",
    "def logaverage(x):\n",
    "    mx = x.max()\n",
    "    return np.log(np.sum(np.exp(x - mx))) + mx - np.log(len(x))\n",
    "\n",
    "log_evidence_straight_line = logaverage(log_likelihood_straight_line)\n",
    "log_evidence_quadratic = logaverage(log_likelihood_quadratic)\n",
    "\n",
    "print('log Evidence for Straight Line Model:', log_evidence_straight_line)\n",
    "print('log Evidence for Quadratic Model:', log_evidence_quadratic)\n",
    "\n",
    "print ('Odds ratio in favour of the Quadratic Model:', np.exp(log_evidence_quadratic - log_evidence_straight_line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic scatter\n",
    "\n",
    "* Let's try adding a different extra parameter to the model: intrinsic scatter. What does this mean? We imagine the model $y$ values to be drawn from a PDF that is conditional on $m$, $b$ and also $s$, the intrinsic scatter of the population. This is the example we worked through in the breakout on Tuesday (although there we didn't do any analytic marginalisation).\n",
    "\n",
    "\n",
    "* This scatter parameter can be inferred from the data as well: in this simple case we can introduce it, along with a \"true\" $y$ value for each data point, and analytically marginalize over the \"true\" y's. \n",
    "\n",
    "\n",
    "* This yields a new (marginalized) likelihood function, which looks as though it has an additional source of uncertainty in the y values - which is what scatter is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def straight_line_with_scatter_log_likelihood(x, y, sigmay, m, b, log_s):\n",
    "    '''\n",
    "    Returns the log-likelihood of drawing data values *y* at\n",
    "    known values *x* given Gaussian measurement noise with standard\n",
    "    deviation with known *sigmay*, where the \"true\" y values have\n",
    "    been drawn from N(mean=m * x + b, variance=(s^2)).\n",
    "\n",
    "    x: list of x coordinates\n",
    "    y: list of y coordinates\n",
    "    sigmay: list of y uncertainties\n",
    "    m: scalar slope\n",
    "    b: scalar line intercept\n",
    "    s: intrinsic scatter, Gaussian std.dev\n",
    "\n",
    "    Returns: scalar log likelihood\n",
    "    '''\n",
    "    s = np.exp(log_s)\n",
    "    V = sigmay**2 + s**2\n",
    "    return (np.sum(np.log(1./(np.sqrt(2.*np.pi*V)))) +\n",
    "            np.sum(-0.5 * (y - (m*x + b))**2 / V))\n",
    "    \n",
    "def straight_line_with_scatter_log_prior(m, b, log_s):\n",
    "    if log_s < np.log(slo) or log_s > np.log(shi):\n",
    "        return -np.inf\n",
    "    return 0.\n",
    "    \n",
    "def straight_line_with_scatter_log_posterior(x,y,sigmay, m,b,log_s):\n",
    "    return (straight_line_with_scatter_log_likelihood(x,y,sigmay,m,b,log_s) +\n",
    "            straight_line_with_scatter_log_prior(m,b,log_s))\n",
    "\n",
    "def straight_line_with_scatter_posterior(x,y,sigmay,m,b,log_s):\n",
    "    return np.exp(straight_line_with_scatter_log_posterior(x,y,sigmay,m,b,log_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial m, b, s\n",
    "m,b,log_s = 2, 20, 0.\n",
    "\n",
    "# step sizes\n",
    "mstep, bstep, log_sstep = 1., 10., 1.\n",
    "\n",
    "# how many steps?\n",
    "nsteps = 30000\n",
    "    \n",
    "schain = []\n",
    "sprobs = []\n",
    "naccept = 0\n",
    "    \n",
    "print 'Running MH for', nsteps, 'steps'\n",
    "L_old    = straight_line_with_scatter_log_likelihood(x, y, sigmay, m, b, log_s)\n",
    "p_old    = straight_line_with_scatter_log_prior(m, b, log_s)\n",
    "prob_old = np.exp(L_old + p_old)\n",
    "\n",
    "for i in range(nsteps):\n",
    "    # step\n",
    "    mnew = m + np.random.normal() * mstep\n",
    "    bnew = b + np.random.normal() * bstep\n",
    "    log_snew = log_s + np.random.normal() * log_sstep\n",
    "\n",
    "    # evaluate probabilities\n",
    "    # prob_new = straight_line_with_scatter_posterior(x, y, sigmay, mnew, bnew, log_snew)\n",
    "\n",
    "    L_new    = straight_line_with_scatter_log_likelihood(x, y, sigmay, mnew, bnew, log_snew)\n",
    "    p_new    = straight_line_with_scatter_log_prior(mnew, bnew, log_snew)\n",
    "    prob_new = np.exp(L_new + p_new)\n",
    "\n",
    "    \n",
    "    if (prob_new / prob_old > np.random.uniform()):\n",
    "        # accept\n",
    "        m = mnew\n",
    "        b = bnew\n",
    "        log_s = log_snew\n",
    "        L_old = L_new\n",
    "        p_old = p_new\n",
    "        prob_old = prob_new\n",
    "        naccept += 1\n",
    "    else:\n",
    "        # Stay where we are; m,b stay the same, and we append them\n",
    "        # to the chain below.\n",
    "        pass\n",
    "        \n",
    "\n",
    "    schain.append((b,m,np.exp(log_s)))\n",
    "    sprobs.append((L_old,p_old))\n",
    "\n",
    "print 'Acceptance fraction:', naccept/float(nsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histograms:\n",
    "import triangle\n",
    "slo,shi = [0,10]\n",
    "triangle.corner(schain, labels=['b','m','s'], range=[(blo,bhi),(mlo,mhi),(slo,shi)],quantiles=[0.16,0.5,0.84],\n",
    "                show_titles=True, title_args={\"fontsize\": 12},\n",
    "                plot_datapoints=True, fill_contours=True, levels=[0.68, 0.95], color='b', bins=20, smooth=1.0);\n",
    "plt.show()\n",
    "\n",
    "# Traces:\n",
    "plt.clf()\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot([b for b,m,s in schain], 'k-')\n",
    "plt.ylabel('b')\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot([m for b,m,s in schain], 'k-')\n",
    "plt.ylabel('m')\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot([s for b,m,s in schain], 'k-')\n",
    "plt.ylabel('s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence for intrinsic scatter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw a large number of prior samples and hope for the best\n",
    "N=50000\n",
    "mm = np.random.uniform(mlo,mhi, size=N)\n",
    "bb = np.random.uniform(blo,bhi, size=N)\n",
    "slo,shi = [0.001,10]\n",
    "log_slo = np.log(slo)\n",
    "log_shi = np.log(shi)\n",
    "log_ss = np.random.uniform(log_slo, log_shi, size=N)\n",
    "\n",
    "log_likelihood_vanilla = np.zeros(N)\n",
    "log_likelihood_scatter = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    log_likelihood_vanilla[i] = straight_line_log_likelihood(x, y, sigmay, mm[i], bb[i])\n",
    "    log_likelihood_scatter[i] = straight_line_with_scatter_log_likelihood(x, y, sigmay, mm[i], bb[i], log_ss[i])\n",
    "\n",
    "def logsum(x):\n",
    "    mx = x.max()\n",
    "    return np.log(np.sum(np.exp(x - mx))) + mx\n",
    "\n",
    "log_evidence_vanilla = logsum(log_likelihood_vanilla) - np.log(N)\n",
    "log_evidence_scatter = logsum(log_likelihood_scatter) - np.log(N)\n",
    "\n",
    "print 'Log evidence vanilla:', log_evidence_vanilla\n",
    "print 'Log evidence scatter:', log_evidence_scatter\n",
    "\n",
    "print 'Odds ratio in favour of the vanilla model:', np.exp(log_evidence_vanilla - log_evidence_scatter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case there is very little to choose between the two models. Both provide comparably good fits to the data, so the only thing working against the scatter model is its extra parameter. However, the prior for `s` is very well -matched to the data (uniform in `log s` corresponds to a `1/s` distribution, favoring small values, and so there is not a very big \"Occam's Razor\" factor in the evidence. Both models are appropriate for this dataset.\n",
    "\n",
    "Incidentally, let's look at a possible approximation (suggested by Lewis & Bridle in the CosmoMC paper) for the evidence - the *posterior* mean log likelihood from our MCMC chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff174dba81c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_log_L_vanilla\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmean_log_L_scatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"No scatter:  Evidence, mean log L, difference: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_evidence_vanilla\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_log_L_vanilla\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_log_L_vanilla\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_evidence_vanilla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"   Scatter:  Evidence, mean log L, difference: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_evidence_scatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_log_L_scatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_log_L_scatter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_evidence_scatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "mean_log_L_vanilla = np.average(np.atleast_1d(probs).T[0])\n",
    "mean_log_L_scatter = np.average(np.atleast_1d(sprobs).T[0])\n",
    "\n",
    "print \"No scatter:  Evidence, mean log L, difference: \",log_evidence_vanilla,mean_log_L_vanilla,(mean_log_L_vanilla - log_evidence_vanilla)\n",
    "print \"   Scatter:  Evidence, mean log L, difference: \",log_evidence_scatter,mean_log_L_scatter,(mean_log_L_scatter - log_evidence_scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the posterior mean log likelihood and the Evidence is the *Shannon information gained* when we updated the prior into the posterior. In both cases we gained about 2 bits of information - perhaps corresponding to approximately 2 good measurements (regardless of the number of parameters being inferred)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Some Other Model Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def likelihood_outliers((m, b, pbad), (x, y, sigmay, sigmabad)):\n",
    "    return np.prod(pbad * 1./(np.sqrt(2.*np.pi)*sigmabad) *\n",
    "                        np.exp(-y**2 / (2.*sigmabad**2))\n",
    "                        + (1.-pbad) * (1./(np.sqrt(2.*np.pi)*sigmay)\n",
    "                                       * np.exp(-(y-(m*x+b))**2/(2.*sigmay**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prior_outliers((m, b, pbad)):\n",
    "    if pbad < 0:\n",
    "        return 0\n",
    "    if pbad > 1:\n",
    "        return 0\n",
    "    return 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob_outliers((m,b,pbad), x,y,sigmay,sigmabad):\n",
    "    return (likelihood_outliers((m,b,pbad), (x,y,sigmay,sigmabad)) *\n",
    "            prior_outliers((m,b,pbad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y,sigmay = data1.T\n",
    "sigmabad = np.std(y)\n",
    "prob_args = (x,y,sigmay,sigmabad)\n",
    "mstep = 0.1\n",
    "bstep = 1.\n",
    "pbadstep = 0.01\n",
    "proposal_args = ((mstep, bstep,pbadstep),)\n",
    "m,b,pbad = 2.2, 30, 0.1\n",
    "mh(prob_outliers, prob_args, gaussian_proposal, proposal_args,\n",
    "    (m,b,pbad), 100000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def likelihood_t((m, b, nu), (x, y, sigmay)):\n",
    "    return np.prod(pbad * 1./(np.sqrt(2.*np.pi)*sigmabad) *\n",
    "                        np.exp(-y**2 / (2.*sigmabad**2))\n",
    "                        + (1.-pbad) * (1./(np.sqrt(2.*np.pi)*sigmay)\n",
    "                                       * np.exp(-(y-(m*x+b))**2/(2.*sigmay**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def complexity_brewer_likelihood((m, b, q), (x, y, sigmay)):\n",
    "    # q: quadratic term\n",
    "    if q < 0:\n",
    "        q = 0\n",
    "    else:\n",
    "        k = 0.01\n",
    "        q = -k * np.log(1 - q)\n",
    "    return np.prod(np.exp(-(y-(b+m*x+q*(x - 150)**2))**2/(2.*sigmay**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def complexity_brewer_prior((m,b,q)):\n",
    "    if q < -1:\n",
    "        return 0.\n",
    "    return 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def complexity_brewer_prob(params, *args):\n",
    "    return complexity_brewer_prior(params) * complexity_brewer_likelihood(params, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y,sigmay = get_data_no_outliers()\n",
    "print 'x', x.min(), x.max()\n",
    "print 'y', y.min(), y.max()\n",
    "y = y + (0.001 * (x-150)**2)\n",
    "prob_args = (x,y,sigmay)\n",
    "mstep = 0.1\n",
    "bstep = 1.\n",
    "qstep = 0.1\n",
    "proposal_args = ((mstep, bstep, qstep, cstep),)\n",
    "m,b,q = 2.2, 30, 0.\n",
    "plt.errorbar(x, y, fmt='.', yerr=sigmay)\n",
    "plt.show()\n",
    "mh(complexity_brewer_prob, prob_args, gaussian_proposal, proposal_args,\n",
    "    (m,b,q), 10000, pnames=['m','b','q']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Copyright (C) Phil Marshall & Dustin Lang\n",
    "\n",
    "This program is free software; you can redistribute it and/or\n",
    "modify it under the terms of the GNU General Public License\n",
    "as published by the Free Software Foundation; either version 2\n",
    "of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the [GNU General Public License](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)\n",
    "along with this program; if not, write to the Free Software\n",
    "Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
